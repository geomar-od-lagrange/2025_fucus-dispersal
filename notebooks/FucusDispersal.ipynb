{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6b0888",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8538e0-0952-424f-8752-77bf07836113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parcels import (\n",
    "        FieldSet,\n",
    "        JITParticle,\n",
    "        ScipyParticle,\n",
    "        ParticleSet,\n",
    "        AdvectionRK4,\n",
    "        AdvectionRK4_3D,\n",
    "        StatusCode,\n",
    ")\n",
    "import parcels\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import dask\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import uniform, randint\n",
    "\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import cmocean\n",
    "\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from time import time\n",
    "import warnings\n",
    "import shapely\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e2c09",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf108bf9-ca4f-459a-b9ef-fbb6b44d4249",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "RNG_seed = 123\n",
    "\n",
    "# Time\n",
    "release_year = 2016\n",
    "start_month = 1\n",
    "start_day = 1\n",
    "end_month = 12\n",
    "end_day = 31\n",
    "max_age_d = 220\n",
    "# timedirection\n",
    "timearrow = 1\n",
    "# Timestep in minutes\n",
    "dt_in_minutes = 15\n",
    "output_dt_in_minutes = 15\n",
    "\n",
    "# Box traits\n",
    "release_depth_sigma = 0\n",
    "\n",
    "n_particles_per_cell = 10\n",
    "\n",
    "repeated_release = True\n",
    "repeatdt_d = 7\n",
    "\n",
    "is_papermill = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c70f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_papermill:\n",
    "        release_year = 2021\n",
    "        start_month = 1\n",
    "        start_day = 1\n",
    "        end_month = 1\n",
    "        end_day = 5\n",
    "        max_age_d = 1\n",
    "        # timedirection\n",
    "        timearrow = 1\n",
    "        # Timestep in minutes\n",
    "        dt_in_minutes = 6*60\n",
    "        output_dt_in_minutes = 12*60\n",
    "\n",
    "\n",
    "        # Box traits\n",
    "        release_depth_sigma = 0\n",
    "\n",
    "        n_particles_per_cell = 10\n",
    "\n",
    "        repeated_release = True\n",
    "        repeatdt_d = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed75f85d",
   "metadata": {},
   "source": [
    "# Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1db691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first release date: 2021-01-01 \n",
      "last date in release timeframe: 2021-01-05 \n",
      "maximum release timeframe in days: 5\n"
     ]
    }
   ],
   "source": [
    "# Get Variables from Parameters\n",
    "first_release_date = np.datetime64(f\"{release_year}-{start_month:02d}-{start_day:02d}\", \"D\")\n",
    "last_date_in_release_timeframe = np.datetime64(f\"{release_year}-{end_month:02d}-{end_day:02d}\", \"D\")\n",
    "maximum_release_timeframe_d = (last_date_in_release_timeframe - first_release_date).astype(int) + 1 \n",
    "\n",
    "print(\"first release date:\", first_release_date, \n",
    "        \"\\nlast date in release timeframe:\", last_date_in_release_timeframe, \n",
    "        \"\\nmaximum release timeframe in days:\", maximum_release_timeframe_d,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7bbcb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeated release dt in days: 2 \n",
      "number of releases: 2 \n",
      "release frame in days: 2\n"
     ]
    }
   ],
   "source": [
    "n_releases = 1\n",
    "if repeated_release:\n",
    "        n_releases = int(np.floor(maximum_release_timeframe_d/repeatdt_d))\n",
    "repeated_release_timeframe_d = (n_releases - 1) * repeatdt_d \n",
    "print(\n",
    "        \"repeated release dt in days:\", repeatdt_d, \n",
    "        \"\\nnumber of releases:\", n_releases, \n",
    "        \"\\nrelease frame in days:\", repeated_release_timeframe_d,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6378a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last release date: 2021-01-03 \n",
      "last modeling date: 2021-01-04\n"
     ]
    }
   ],
   "source": [
    "release_timeframe_d = min(repeated_release_timeframe_d, maximum_release_timeframe_d)\n",
    "last_release_date = first_release_date + np.timedelta64(release_timeframe_d, \"D\")\n",
    "last_modeling_date = last_release_date + np.timedelta64(max_age_d - 1, \"D\")\n",
    "last_modeling_date = last_release_date + np.timedelta64(max_age_d, \"D\")\n",
    "# last_modeling_date = last_date_in_release_timeframe + np.timedelta64(max_age_d + 1, \"D\")\n",
    "\n",
    "print(\n",
    "        \"last release date:\", last_release_date, \n",
    "        \"\\nlast modeling date:\", last_modeling_date,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554c1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_day_in_year = np.datetime64(f\"{release_year}-01-01\", \"D\")\n",
    "\n",
    "last_year = last_modeling_date.astype(datetime).year\n",
    "years = np.arange(release_year, last_year+1)\n",
    "\n",
    "runtime_in_days = (last_modeling_date - first_release_date).tolist()\n",
    "calc_dt = np.timedelta64(dt_in_minutes, \"m\").tolist()\n",
    "output_dt = np.timedelta64(output_dt_in_minutes, \"m\").tolist()\n",
    "\n",
    "np.random.seed(RNG_seed)\n",
    "save_path = f\"/gxfs_work/geomar/smomw597/2025_Fucus/output/Trajectories/{release_year}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b017555",
   "metadata": {},
   "source": [
    "## Get file names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966b5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_release_date_str = first_release_date.astype(str).replace(\"-\",\"\")\n",
    "last_release_date_str = last_release_date.astype(str).replace(\"-\",\"\")\n",
    "last_modeling_date_str = last_modeling_date.astype(str).replace(\"-\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc59580c-c63f-41b8-a8a3-70518b19a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish folder paths\n",
    "path_orig_files = Path(\"/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/\")\n",
    "path_static_files = Path(\"/gxfs_work/geomar/smomw122/bsh_operationalmodel_data\")\n",
    "path_static_fine = path_static_files / \"static_file_fine\"\n",
    "path_static_coarse = path_static_files / \"static_file_coarse\"\n",
    "\n",
    "# get static file paths\n",
    "sigma_file_fine = path_static_fine / \"sigma_file_fine.nc\"\n",
    "lonlat_file_fine = path_static_fine / \"lonlat_file_fine.nc\"\n",
    "\n",
    "sigma_file_coarse = path_static_coarse / \"sigma_file_coarse.nc\"\n",
    "lonlat_file_coarse = path_static_coarse / \"lonlat_file_coarse.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a95cc901",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_subfolder_fine = list(map(lambda x: \"c_file_fine_\" + str(x), years))\n",
    "current_subfolder_coarse = list(map(lambda x: \"c_file_coarse_\" + str(x), years))\n",
    "\n",
    "salt_temp_subfolder_fine = list(map(lambda x: \"t_file_fine_\" + str(x), years))\n",
    "salt_temp_subfolder_coarse = list(map(lambda x: \"t_file_coarse_\" + str(x), years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247afa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(subfolder_array, years, path):\n",
    "        files_list = list(\n",
    "                itertools.chain.from_iterable(\n",
    "                        [\n",
    "                                sorted(path.glob(f\"{subfolder}/*\")) \n",
    "                                for subfolder in subfolder_array\n",
    "                        ]\n",
    "                )\n",
    "        )\n",
    "        last_file = (last_modeling_date - np.timedelta64(1,\"D\")).astype(str).replace(\"-\",\"\")\n",
    "        n_files_before_first_file = (first_release_date - first_day_in_year).astype(int) * 4\n",
    "        year_str = str(years[0])\n",
    "        first_character_date = files_list[0].name.find(year_str)\n",
    "        last_character_date = files_list[0].name.find(\"_000_006.nc\")-2\n",
    "\n",
    "        del files_list[:n_files_before_first_file]\n",
    "\n",
    "        for file in reversed(files_list):\n",
    "                if last_file + \"00\" in file.name:\n",
    "                        print(last_file)\n",
    "                        break\n",
    "                if file.name[first_character_date:last_character_date] >= last_file:\n",
    "                        files_list.remove(file)\n",
    "        return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ab23b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(subfolder_array, years, path):\n",
    "        files_list = list(\n",
    "                itertools.chain.from_iterable(\n",
    "                        [\n",
    "                                sorted(path.glob(f\"{subfolder}/*\")) \n",
    "                                for subfolder in subfolder_array\n",
    "                        ]\n",
    "                )\n",
    "        )\n",
    "        n_files_before_first_file = (first_release_date - first_day_in_year).astype(int) * 4\n",
    "        year_str = str(years[0])\n",
    "        first_character_date = files_list[0].name.find(year_str)\n",
    "        last_character_date = files_list[0].name.find(\"_000_006.nc\")-2\n",
    "\n",
    "        del files_list[:n_files_before_first_file]\n",
    "\n",
    "        for file in reversed(files_list):\n",
    "                if file.name[first_character_date:last_character_date+2] > last_modeling_date_str + \"00\":\n",
    "                        files_list.remove(file)\n",
    "                if file.name[first_character_date:last_character_date+2] == last_modeling_date_str + \"00\":\n",
    "                        break\n",
    "                        \n",
    "        return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68e6b4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files per list: 13\n"
     ]
    }
   ],
   "source": [
    "# get file list paths\n",
    "current_files_fine = get_file_list(current_subfolder_fine, years, path_orig_files)\n",
    "salt_temp_files_fine = get_file_list(salt_temp_subfolder_fine, years, path_orig_files)\n",
    "\n",
    "current_files_coarse = get_file_list(current_subfolder_coarse, years, path_orig_files)\n",
    "salt_temp_files_coarse = get_file_list(salt_temp_subfolder_coarse, years, path_orig_files)\n",
    "\n",
    "print(\"Number of files per list:\", len(salt_temp_files_coarse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1a3f594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010100_000_006.nc'),\n",
       " PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010106_000_006.nc'),\n",
       " PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010112_000_006.nc'),\n",
       " PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010118_000_006.nc'),\n",
       " PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010200_000_006.nc'),\n",
       " PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010206_000_006.nc'),\n",
       " PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010212_000_006.nc'),\n",
       " PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010218_000_006.nc'),\n",
       " PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010300_000_006.nc'),\n",
       " PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010306_000_006.nc'),\n",
       " PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010312_000_006.nc'),\n",
       " PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010318_000_006.nc'),\n",
       " PosixPath('/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/t_file_coarse_2021/t_file_coarse_2021010400_000_006.nc')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salt_temp_files_coarse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77d776f-3a5e-4199-a6a6-29d64575959b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbfd8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sigma_files = Path(\"/gxfs_work/geomar/smomw597/2025_Fucus/output/sigma/\")\n",
    "sigma_file_fine = path_sigma_files / \"sigma_fine.nc\"\n",
    "sigma_file_coarse = path_sigma_files / \"sigma_coarse.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7524c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_position_in_cell(x_rel: float, y_rel: float, cell: shapely.Polygon):\n",
    "        (x0, y0), (x1, y1), (x2, y2), (x3, y3), (x4, y4) = cell.exterior.coords\n",
    "        ex = (x3-x0, y3-y0)\n",
    "        ey = (x1-x0, y1-y0)\n",
    "        return x0 + x_rel * ex[0] + y_rel * ey[0], y0 + x_rel * ex[1] + y_rel * ey[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e02fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping parameters\n",
    "suptitle_size = 20\n",
    "title_size = 16\n",
    "mapsize = (16, 8)\n",
    "extent = (2.5, 27.5, 52, 62)\n",
    "lonmid = np.mean(extent[:2])\n",
    "latmid = np.mean(extent[:2])\n",
    "map_projection = ccrs.Stereographic(central_longitude=lonmid, central_latitude=latmid)\n",
    "def basemap(ax):\n",
    "        ax.set_extent(extent, ccrs.PlateCarree())\n",
    "        ax.add_feature(cartopy.feature.LAND)\n",
    "        ax.add_feature(cartopy.feature.COASTLINE)\n",
    "        ax.gridlines(draw_labels=[\"left\", \"bottom\"], y_inline=False)\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60202220-d0d0-4e8e-8776-f1fad319fe00",
   "metadata": {},
   "source": [
    "# Construct release locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f9bfadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of release cells: 872 \n",
      "number particles per cell: 10 \n",
      "number of particles per release: 8720 \n",
      "total number of particles: 17440 ['2021-01-01T12' '2021-01-03T12']\n"
     ]
    }
   ],
   "source": [
    "release_area_path = Path(\"/gxfs_work/geomar/smomw597/2025_Fucus/2025_fucus-dispersal/Fucus_location_shp\")\n",
    "gdf_release_area = gpd.read_file(Path(release_area_path,\"REDLIST_SIS_Macrophytes.geojson\"))\n",
    "\n",
    "gdf_release_area = (\n",
    "        gdf_release_area\n",
    "        .drop(gdf_release_area[gdf_release_area.F_vesiculo==0].index)\n",
    "        .to_crs(crs=ccrs.Geodetic())\n",
    "        .assign(area_m2 = gdf_release_area.to_crs(crs=ccrs.AlbersEqualArea()).area)\n",
    ")[[\"CELLCODE\", \"F_vesiculo\", \"geometry\", \"CELLID\"]]\n",
    "\n",
    "# Get total number of particles\n",
    "n_release_cells = gdf_release_area.shape[0]\n",
    "n_particles = n_release_cells * n_particles_per_cell\n",
    "\n",
    "repeated_release = True\n",
    "\n",
    "start_time = first_release_date + np.timedelta64(12, \"h\")\n",
    "start_times = [start_time] * n_particles\n",
    "\n",
    "if repeated_release:\n",
    "        start_times = [list(start_times + np.timedelta64(repeatdt_d, \"D\") * week) for week in range(n_releases)]\n",
    "        start_times = list(itertools.chain.from_iterable(start_times))\n",
    "        n_total_particles = n_particles * n_releases\n",
    "else:\n",
    "        n_total_particles = n_particles\n",
    "\n",
    "print(\n",
    "        \"number of release cells:\", n_release_cells, \n",
    "        \"\\nnumber particles per cell:\", n_particles_per_cell, \n",
    "        \"\\nnumber of particles per release:\", n_particles,\n",
    "        \"\\ntotal number of particles:\", n_total_particles,\n",
    "        np.unique(start_times),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cd0593c-4c6f-41e3-8544-6f5728319123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output path: /gxfs_work/geomar/smomw597/2025_Fucus/output/Trajectories/2021/TEST_Nested_20210101-20210103_dt720min_N17440_seed123.zarr\n"
     ]
    }
   ],
   "source": [
    "filename_time = f\"{first_release_date_str}-{last_release_date_str}_dt{output_dt_in_minutes}min\"\n",
    "output_filename = f\"Nested_{filename_time}_N{n_total_particles}_seed{RNG_seed}.zarr\"\n",
    "# define Output path and name\n",
    "if is_papermill:\n",
    "        output_filename = str(\"PPmill_\" + output_filename)\n",
    "else:\n",
    "        output_filename = str(\"TEST_\" + output_filename)\n",
    "\n",
    "output_path = Path(save_path, output_filename)\n",
    "print(\"Output path:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ec182b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random cell and a random position within cell\n",
    "release_lons, release_lats, cellcode = list(\n",
    "        zip(\n",
    "                *[\n",
    "                        relative_position_in_cell(\n",
    "                                rand_x, rand_y, gdf_release_area.iloc[rand_cell].geometry\n",
    "                        )\n",
    "                        + (gdf_release_area.iloc[rand_cell].CELLCODE,)\n",
    "                        for rand_x, rand_y, rand_cell in zip(\n",
    "                                uniform(0, 1, size=n_total_particles),\n",
    "                                uniform(0, 1, size=n_total_particles),\n",
    "                                randint(0, len(gdf_release_area), size=n_total_particles),\n",
    "                        )\n",
    "                ]\n",
    "        )\n",
    ")\n",
    "release_depths = np.full_like(release_lons, release_depth_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98dc9d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles_cellID_list = []\n",
    "for cell in cellcode:\n",
    "        n_particles_cellID_list.extend(gdf_release_area[gdf_release_area.CELLCODE == cell].CELLID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca28668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #make sigmafiles without NaN\n",
    "# path_sigma_files = Path(\"/gxfs_work/geomar/smomw597/2025_Fucus/output/sigma/\")\n",
    "# path_sigma_files.mkdir(exist_ok=True)\n",
    "\n",
    "# ds_sigma_fine = xr.open_dataset(sigma_file_fine)\n",
    "# ds_sigma_fine = ds_sigma_fine.assign(sigma = xr.where(ds_sigma_fine.layer_number == 1, 0, ds_sigma_fine.sigma))\n",
    "# ds_sigma_fine.to_netcdf(path_sigma_files/\"sigma_fine.nc\")\n",
    "\n",
    "# ds_sigma_coarse = xr.open_dataset(sigma_file_coarse)\n",
    "# ds_sigma_coarse = ds_sigma_coarse.assign(sigma = xr.where(ds_sigma_coarse.layer_number == 1, 0, ds_sigma_coarse.sigma))\n",
    "# ds_sigma_coarse.to_netcdf(path_sigma_files/\"sigma_coarse.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218eff6-4b62-422b-84ee-18b3282a942f",
   "metadata": {},
   "source": [
    "# Parcels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e3d303-1652-486e-b448-c9a387e2f3a1",
   "metadata": {},
   "source": [
    "## Custom Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6f0f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_error_particle(particle, fieldset, time):\n",
    "    if particle.state >= 50:  # This captures all Errors\n",
    "        particle.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32c6537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish aging for particles\n",
    "def aging_kernel(particle, fieldset, time):\n",
    "    particle.age_sec += particle.dt\n",
    "    max_age_sec = fieldset.max_age_d * 60 * 60 * 24\n",
    "    if particle.age_sec > max_age_sec:\n",
    "        particle.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f04e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvectionRK4_2D_SIGMABSH(particle, fieldset, time):\n",
    "        time0 = time\n",
    "        sig0 = particle.depth\n",
    "        lat0 = particle.lat\n",
    "        lon0 = particle.lon\n",
    "\n",
    "        (u1, v1) = fieldset.UV[time0, sig0, lat0, lon0]  # horizontal velocities in deg/s\n",
    "\n",
    "        s1 = fieldset.S[time0, sig0, lat0, lon0]\n",
    "        t1 = fieldset.T[time0, sig0, lat0, lon0]\n",
    "\n",
    "        time1 = time0 + 0.5 * particle.dt\n",
    "\n",
    "        lat1 = lat0 + v1 * 0.5 * particle.dt\n",
    "        lon1 = lon0 + u1 * 0.5 * particle.dt\n",
    "\n",
    "        sig1 = sig0\n",
    "        (u2, v2) = fieldset.UV[time1, sig1, lat1, lon1]\n",
    "\n",
    "        time2 = time0 + 0.5 * particle.dt\n",
    "        lat2 = lat0 + v2 * 0.5 * particle.dt\n",
    "        lon2 = lon0 + u2 * 0.5 * particle.dt\n",
    "        \n",
    "        sig2 = sig0\n",
    "        (u3, v3) = fieldset.UV[time2, sig2, lat2, lon2]\n",
    "\n",
    "        time3 = time0 + particle.dt\n",
    "\n",
    "        lat3 = lat0 + v3 * particle.dt\n",
    "        lon3 = lon0 + u3 * particle.dt\n",
    "\n",
    "        sig3 = sig0\n",
    "        (u4, v4) = fieldset.UV[time3, sig3, lat3, lon3]\n",
    "\n",
    "        lon4 = lon0 + (u1 + 2 * u2 + 2 * u3 + u4) / 6 * particle.dt\n",
    "        lat4 = lat0 + (v1 + 2 * v2 + 2 * v3 + v4) / 6 * particle.dt\n",
    "\n",
    "        particle_dlon += lon4 - lon0\n",
    "        particle_dlat += lat4 - lat0\n",
    "\n",
    "        particle.u = u1\n",
    "        particle.v = v1\n",
    "        particle.S = s1\n",
    "        particle.T = t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5bbdace",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_kernel = [AdvectionRK4_2D_SIGMABSH, delete_error_particle]\n",
    "custom_kernel = [AdvectionRK4_2D_SIGMABSH, aging_kernel]\n",
    "custom_kernel = [AdvectionRK4_2D_SIGMABSH]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd617f-b9d6-4c94-ab20-91698a973ba9",
   "metadata": {},
   "source": [
    "## Fieldset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "017bba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_dict_lonlat = dict(lon=\"lon\", lat=\"lat\")\n",
    "dim_dict_lonlat_time = dict(**dim_dict_lonlat, time=\"time\")\n",
    "dim_dict_lonlat_time_depth = dict(**dim_dict_lonlat_time, depth=\"sigma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7996e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare reading of variables\n",
    "current_variable_ID = [\"U\", \"V\"]\n",
    "salt_temp_variable_ID = [\"S\", \"T\"]\n",
    "current_interp_methods = [\"cgrid_velocity\", \"cgrid_velocity\"]\n",
    "salt_temp_interp_methods = [\"cgrid_tracer\", \"cgrid_tracer\"]\n",
    "\n",
    "dimensions = [dim_dict_lonlat_time_depth, dim_dict_lonlat_time_depth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54b97633",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_variables = dict(zip(current_variable_ID, [\"uvel\", \"vvel\"]))\n",
    "current_interp_method = dict(zip(current_variable_ID, current_interp_methods))\n",
    "current_dimensions = dict(zip(current_variable_ID, dimensions))\n",
    "\n",
    "salt_temp_variables = dict(zip(salt_temp_variable_ID, [\"salt\", \"temp\"]))\n",
    "salt_temp_interp_methods = dict(zip(salt_temp_variable_ID, salt_temp_interp_methods))\n",
    "salt_temp_dimensions = dict(zip(salt_temp_variable_ID, dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "595575d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _time = xr.open_mfdataset(current_files_fine).time\n",
    "# _t0 = _time.isel(time=0).values\n",
    "# _t_ref = np.datetime64(\"1900-01-01T00:00:00\")\n",
    "# _dt = np.timedelta64(15 * 60, \"s\")\n",
    "# timesteps = np.arange(len(_time)) * _dt + round((_t0 - _t_ref)/_dt) * _dt + _t_ref\n",
    "# timesteps = timesteps.reshape(-1, 24)\n",
    "# timesteps_list = list(map(list,timesteps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df5091",
   "metadata": {},
   "source": [
    "### Fine Fieldsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1f58596",
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat_dict_fine = dict(lon=lonlat_file_fine, lat=lonlat_file_fine)\n",
    "\n",
    "current_dict_fine = dict(\n",
    "        lonlat_dict_fine, \n",
    "        depth=sigma_file_fine,\n",
    "        data=current_files_fine,\n",
    ")\n",
    "salt_temp_dict_fine = dict(\n",
    "        lonlat_dict_fine,\n",
    "        depth=sigma_file_fine,\n",
    "        data=salt_temp_files_fine,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42fa98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fine fieldset for all the other Values\n",
    "current_filenames_fine = dict(U=current_dict_fine, V=current_dict_fine)\n",
    "\n",
    "current_fieldset_fine = FieldSet.from_netcdf(\n",
    "        filenames=current_filenames_fine,\n",
    "        variables=current_variables,\n",
    "        dimensions=current_dimensions,\n",
    "        interp_method=current_interp_method,\n",
    "        allow_time_extrapolation=False,\n",
    "        gridindexingtype=\"nemo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35a98d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fine fieldset for Temp and salinity\n",
    "salt_temp_filenames_fine = dict(S=salt_temp_dict_fine, T=salt_temp_dict_fine)\n",
    "\n",
    "salt_temp_fieldset_fine = FieldSet.from_netcdf(\n",
    "        filenames=salt_temp_filenames_fine,\n",
    "        variables=salt_temp_variables,\n",
    "        dimensions=salt_temp_dimensions,\n",
    "        interp_method=salt_temp_interp_methods,\n",
    "        allow_time_extrapolation=False,\n",
    "        gridindexingtype=\"nemo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a286b",
   "metadata": {},
   "source": [
    "### Coarse Fieldsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d35ca910",
   "metadata": {},
   "outputs": [],
   "source": [
    "lonlat_dict_coarse = dict(lon=lonlat_file_coarse, lat=lonlat_file_coarse)\n",
    "\n",
    "current_dict_coarse = dict(\n",
    "        lonlat_dict_coarse, \n",
    "        depth=sigma_file_coarse,\n",
    "        data=current_files_coarse,\n",
    ")\n",
    "salt_temp_dict_coarse = dict(\n",
    "        lonlat_dict_coarse,\n",
    "        depth=sigma_file_coarse,\n",
    "        data=salt_temp_files_coarse,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d99fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build coarse fieldset for all the other Values\n",
    "current_filenames_coarse = dict(U=current_dict_coarse, V=current_dict_coarse)\n",
    "\n",
    "current_fieldset_coarse = FieldSet.from_netcdf(\n",
    "        filenames=current_filenames_coarse,\n",
    "        variables=current_variables,\n",
    "        dimensions=current_dimensions,\n",
    "        interp_method=current_interp_method,\n",
    "        allow_time_extrapolation=False,\n",
    "        gridindexingtype=\"nemo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c782f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build coarse fieldset for Temp and salinity\n",
    "salt_temp_filenames_coarse = dict(S=salt_temp_dict_coarse, T=salt_temp_dict_coarse)\n",
    "\n",
    "salt_temp_fieldset_coarse = FieldSet.from_netcdf(\n",
    "        filenames=salt_temp_filenames_coarse,\n",
    "        variables=salt_temp_variables,\n",
    "        dimensions=salt_temp_dimensions,\n",
    "        interp_method=salt_temp_interp_methods,\n",
    "        allow_time_extrapolation=False,\n",
    "        gridindexingtype=\"nemo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6a38b",
   "metadata": {},
   "source": [
    "### Nested fieldset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a95a4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Nested field from the fine and coarse fields\n",
    "U_nested_field = parcels.NestedField(\"U\", [current_fieldset_fine.U, current_fieldset_coarse.U])\n",
    "V_nested_field = parcels.NestedField(\"V\", [current_fieldset_fine.V, current_fieldset_coarse.V])\n",
    "\n",
    "nested_fieldset = FieldSet(U_nested_field, V_nested_field)\n",
    "\n",
    "S_nested_field = parcels.NestedField(\"S\", [salt_temp_fieldset_fine.S, salt_temp_fieldset_coarse.S])\n",
    "T_nested_field = parcels.NestedField(\"T\", [salt_temp_fieldset_fine.T, salt_temp_fieldset_coarse.T])\n",
    "\n",
    "nested_fieldset.add_field(S_nested_field)\n",
    "nested_fieldset.add_field(T_nested_field)\n",
    "# nested_fieldset.add_constant(\"max_age_d\", max_age_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9deb318-548c-41d9-89d7-b69f011fc934",
   "metadata": {},
   "source": [
    "## Create Particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c8cbe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish particle variables\n",
    "particle_variables = (\n",
    "        \"u\", \"v\",\n",
    "        \"S\", \"T\",\n",
    ")\n",
    "sample_particle = parcels.JITParticle.add_variables(particle_variables)\n",
    "sample_particle = sample_particle.add_variable(\n",
    "        parcels.Variable(\"CellID\", initial=n_particles_cellID_list),\n",
    ")\n",
    "# sample_particle = sample_particle.add_variable(\"age_sec\", initial=0)\n",
    "# sample_particle = parcels.JITParticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f781690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pset = ParticleSet(\n",
    "        fieldset=nested_fieldset,\n",
    "        pclass=sample_particle,\n",
    "        lat=release_lats,\n",
    "        lon=release_lons,\n",
    "        depth=release_depths,\n",
    "        time=start_times,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d08a46f2-754c-4dd7-9048-f0813cbe79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Outputparameters\n",
    "output_chunks = (n_total_particles, int(24 * 60 / output_dt_in_minutes))\n",
    "output_particle_file = pset.ParticleFile(\n",
    "        name=output_path,\n",
    "        outputdt=output_dt,\n",
    "        chunks=output_chunks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce0039",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc4988-badf-4441-bbb0-c06ea3c56995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Output files are stored in /gxfs_work/geomar/smomw597/2025_Fucus/output/Trajectories/2021/TEST_Nested_20210101-20210103_dt720min_N17440_seed123.zarr.\n",
      " 20%|█████████████████████████████████████████▉                                                                                                                                                                   | 53099.712/259200.0 [00:37<02:07, 1612.71it/s]"
     ]
    }
   ],
   "source": [
    "# Execute Simulation\n",
    "pset.execute(\n",
    "    custom_kernel,\n",
    "    dt=calc_dt,\n",
    "    runtime=runtime_in_days,    \n",
    "    output_file=output_particle_file,\n",
    "    verbose_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527eb76d-f318-4574-911f-b98d004c2b84",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f95538",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668b493-e0f2-415e-9c30-741288bf5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_trajectories = xr.open_zarr(output_path).compute()\n",
    "ds_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_trajectories.lat.diff(\"obs\").plot.hist(bins=101);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a943a",
   "metadata": {},
   "source": [
    "## Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be87d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_lat = ds_trajectories.isel(obs=0).lat\n",
    "first_lon = ds_trajectories.isel(obs=0).lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b54cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_valid_obs = ds_trajectories.obs.where(ds_trajectories.lon.notnull()).max('obs').astype(int)\n",
    "last_step = ds_trajectories.isel(obs=last_valid_obs).compute()\n",
    "\n",
    "last_lon = last_step.lon\n",
    "last_lat = last_step.lat\n",
    "\n",
    "last_step.to_dataframe().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf276e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_dead = ds_trajectories.where(first_lon == last_lon, drop=True)\n",
    "traj_moving = ds_trajectories.where(first_lon != last_lon, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428bbcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Files\n",
    "data_path_orig_files = Path(\"/gxfs_work/geomar/smomw400/bsh_operationalmodel_data/\")\n",
    "\n",
    "z_files_fine = sorted(data_path_orig_files.glob(f\"z_file_fine_{release_year}/*\"))[first_file_count:last_file_count]\n",
    "z_files_coarse = sorted(data_path_orig_files.glob(f\"z_file_coarse_{release_year}/*\"))[first_file_count:last_file_count]\n",
    "# open eta and H0 files\n",
    "ds_eta_fine = xr.open_dataset(z_files_fine[0])\n",
    "ds_eta_coarse = xr.open_dataset(z_files_coarse[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48af69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,18))\n",
    "ax = fig.add_subplot(\n",
    "    1,1,1,\n",
    "    projection= ccrs.PlateCarree(),\n",
    ")\n",
    "\n",
    "ds_eta_fine.elev.isel(time=0).plot.imshow(\n",
    "    cmap=cmocean.cm.deep, ax=ax, \n",
    ")\n",
    "ds_eta_coarse.elev.isel(time=0).plot.imshow(\n",
    "    cmap=cmocean.cm.deep, ax=ax, \n",
    ")\n",
    "\n",
    "# gdf_release_area.geometry.plot(ax=ax, color=\"gold\", alpha=1)\n",
    "# ax.scatter(traj_moving.lon, traj_moving.lat, c=\"b\", s=1)\n",
    "# ax.scatter(release_lons, release_lats, s=1, c=\"r\")\n",
    "\n",
    "ax.scatter(traj_dead.isel(obs=0).lon, traj_dead.isel(obs=0).lat, c=\"r\", s=5)\n",
    "ax.scatter(traj_moving.isel(obs=0).lon, traj_moving.isel(obs=0).lat, c=\"k\", s=5)\n",
    "\n",
    "# ax.scatter(ds_trajectories.lon, ds_trajectories.lat, c=\"b\", cmap=\"rainbow\", s=1, )\n",
    "ax.coastlines()\n",
    "ax.gridlines(draw_labels=True)\n",
    "plt.xlim(16.5,22)\n",
    "plt.ylim(59.5,64)\n",
    "# plt.xlim(21,25)\n",
    "# plt.ylim(57.5,59)\n",
    "# plt.xlim(9,25)\n",
    "# plt.ylim(53.5,62.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,12))\n",
    "ax = fig.add_subplot(\n",
    "    1,1,1,\n",
    "    projection= ccrs.PlateCarree(),\n",
    ")\n",
    "\n",
    "ds_eta_fine.elev.isel(time=0).plot(\n",
    "    cmap=cmocean.cm.deep, ax=ax, \n",
    ")\n",
    "ds_eta_coarse.elev.isel(time=0).plot(\n",
    "    cmap=cmocean.cm.deep, ax=ax, \n",
    ")\n",
    "\n",
    "gdf_release_area.geometry.plot(ax=ax, color=\"gold\", alpha=1)\n",
    "ax.scatter(release_lons, release_lats, c=\"magenta\", s=1)\n",
    "\n",
    "ax.scatter(traj_dead.isel(obs=0).lon, traj_dead.isel(obs=0).lat, c=\"r\", s=5)\n",
    "ax.scatter(traj_moving.isel(obs=0).lon, traj_moving.isel(obs=0).lat, c=\"k\", s=5)\n",
    "\n",
    "ax.coastlines()\n",
    "ax.gridlines(draw_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ganz_tot = np.unique(traj_dead.isel(obs=0).CellID.values)[\n",
    "        np.invert(\n",
    "                np.in1d(\n",
    "                        np.unique(traj_dead.isel(obs=0).CellID.values),\n",
    "                        np.unique(traj_moving.isel(obs=0).CellID.values)\n",
    "                )\n",
    "        )\n",
    "]\n",
    "len(ganz_tot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
